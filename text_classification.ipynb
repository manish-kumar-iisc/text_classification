{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Cool stuff"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.functional\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics import accuracy_score\nimport sklearn\nimport numpy\n\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nimport keras.layers as layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sst1_xt=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-1.x.train.txt\",header=None)\nsst1_yt=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-1.y.train.txt\",header=None)\nsst1_xd=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-1.x.dev.txt\",header=None)\nsst1_yd=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-1.y.dev.txt\",header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the corpus using nltk"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\ndef Clean(data):\n    for i,j in enumerate(data):\n        text=str(j).lower()\n        text=re.sub('\\w*\\d\\w*','',text)\n        text = re.sub('\\[.*?\\]', '', text)\n        data[i]=tokenizer.tokenize(text)\n    return data\n\nsst1_xt[0]=Clean(sst1_xt[0])\nsst1_xt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_zs=\"../input/assign-4-labeld/unlabeled_x.txt\"\n\nwith open(train_zs, 'rb') as f:\n        train_zs_lines = f.read().splitlines()\ntrain_zs_lines = [x.decode('ISO-8859-1') for x in train_zs_lines]\na=pd.DataFrame(train_zs_lines)\na[0]=Clean(a[0])\nz=[]\nfor samp in a[0]:\n    z.append(samp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization & stemmatization which one?\nLemmatization i will use "},{"metadata":{"trusted":true},"cell_type":"code","source":"def Lemmatization(data):\n    lemmatizer=WordNetLemmatizer()\n    for i,sentence in enumerate(data):\n        data[i]=[lemmatizer.lemmatize(words) for words in sentence]\n    \n    return data\nsst1_xt[0]=Lemmatization(sst1_xt[0])\nsst1_xt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"def StopWords(data):\n    stop_words=set(stopwords.words('english'))\n    for i,sentence in enumerate(data):\n        data[i]=[word for word in sentence if not word in stop_words]\n    return data\n\nsst1_xt[0]=StopWords(sst1_xt[0])\nsst1_xt.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using glove vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"def GloveModel():\n    path=\"../input/nlpword2vecembeddingspretrained/glove.6B.300d.txt\"\n    file=open(path,\"r\")\n    glovemodel={}\n    for line in file:\n        splitline=line.split()\n        word=splitline[0]\n        wordemb=np.array([float(value) for value in splitline[1:]])\n        glovemodel[word]=wordemb\n    print(f\"word_loaded successfully\")\n    return glovemodel\nglovemodel=GloveModel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=0\nsst1_xd[0]=Clean(sst1_xd[0])\nsst1_xd[0]=Lemmatization(sst1_xd[0])\nsst1_xd[0]=StopWords(sst1_xd[0])\nfor sentence in sst1_xd[0]:\n    a=max(a,len(sentence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Taking average of words for sentence vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"def SentenceVector(data):\n    for i,sentence in enumerate(data):\n        vector=np.zeros(300)\n        for word in sentence:\n            if word in glovemodel.keys():\n                vector+=glovemodel[word]\n        if(len(sentence)>0):\n            data[i]=vector/len(sentence)\n        else:\n            data[i]=vector\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting to sentence matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence=np.zeros((sst1_xt.shape[0],300))\nfor i,j in enumerate(data):\n    sentence[i]=j[0:300]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentence matrix for test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"sst1_xd=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-1.x.dev.txt\",header=None)\nsst1_xd[0]=Clean(sst1_xd[0])\nsst1_xd[0]=Lemmatization(sst1_xd[0])\nsst1_xd[0]=StopWords(sst1_xd[0])\nsst1_xd[0]=SentenceVector(sst1_xd[0])\nsentence1=np.zeros((sst1_xd.shape[0],300))\nfor i,j in enumerate(sst1_xd[0]):\n    sentence1[i]=j[0:300]\nsentence1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SST-2 dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sst2():\n    ## reading data\n    sst2_xt=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-2.x.train.txt\",header=None)\n    sst2_yt=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-2.y.train.txt\",header=None)\n    sst2_xd=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-2.x.dev.txt\",header=None)\n    sst2_yd=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-2.y.dev.txt\",header=None)\n    \n    ## cleaning data & getting vectors\n    \n    # for dev data\n    sst2_xd[0]=Clean(sst2_xd[0])\n    sst2_xd[0]=Lemmatization(sst2_xd[0])\n    sst2_xd[0]=StopWords(sst2_xd[0])\n    sst2_xd[0]=SentenceVector(sst2_xd[0])\n    sentence1=np.zeros((sst2_xd.shape[0],300))\n    \n    #for train data\n    sst2_xt[0]=Clean(sst2_xt[0])\n    sst2_xt[0]=Lemmatization(sst2_xt[0])\n    sst2_xt[0]=StopWords(sst2_xt[0])\n    sst2_xt[0]=SentenceVector(sst2_xt[0])\n    sentence=np.zeros((sst2_xt.shape[0],300))\n    \n    ## creating vector matrix \n    sentence1=np.zeros((sst2_xd.shape[0],300))\n    for i,j in enumerate(sst2_xd[0]):\n        sentence1[i]=j[0:300]\n    sentence1.shape\n    \n    sentence=np.zeros((sst2_xt.shape[0],300))\n    for i,j in enumerate(sst2_xt[0]):\n        sentence[i]=j[0:300]\n    sentence.shape\n    \n    \n    \n    x_train=sentence\n    y_train=pd.get_dummies(sst2_yt)\n    x_test=sentence1\n    y_test=pd.get_dummies(sst2_yd)\n\n    model=Sequential()\n    model.add(Dense(50,input_dim=x_train.shape[1],activation='relu'))\n    model.add(Dense(20,activation='relu'))\n    model.add(Dense(10,activation='relu'))\n    model.add(Dense(y_train.shape[1],activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    model.fit(x_train,y_train,epochs=14,batch_size=60)\n    _,accuracy=model.evaluate(x_train,y_train)\n\n    output=model.predict(x_test)\n\n    result=np.zeros((output.shape[0],output.shape[1]))\n    for i,j in enumerate(output):\n        result[i][numpy.argmax(output[i])]=1\n    accuracy=accuracy_score(y_test,result)\n    print(f\"test accuracy on sst2 dataset: {accuracy}\")\n    return model\n\nmodelsst2=sst2()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CR"},{"metadata":{"trusted":true},"cell_type":"code","source":"def CR():\n    ## reading data\n    file1 = open('/kaggle/input/dlnlp-assig2/DLNLP assig2/CR.x.train.txt', 'r') \n    lists = file1.readlines()\n    sst2_xt=[]\n    for i in range(len(lists)):\n        sst2_xt.append(lists[i].strip('\\n'))\n    sst2_xt=pd.DataFrame(sst2_xt)\n\n    file2 = open('/kaggle/input/dlnlp-assig2/DLNLP assig2/CR.y.train.txt', 'r') \n    listsy = file2.readlines()\n    sst2_yt=[]\n    for i in range(len(listsy)):\n        sst2_yt.append(listsy[i].strip('\\n'))\n    sst2_yt=pd.DataFrame(sst2_yt)\n    \n    sst2_xt[0]=Clean(sst2_xt[0])\n    sst2_xt[0]=Lemmatization(sst2_xt[0])\n    sst2_xt[0]=StopWords(sst2_xt[0])\n    sst2_xt[0]=SentenceVector(sst2_xt[0])\n    sentence=np.zeros((sst2_xt.shape[0],300))\n    sst2_xt,sst2_xd,sst2_yt,sst2_yd=sklearn.model_selection.train_test_split(sst2_xt,sst2_yt,test_size=0.001)\n    \n    ## creating vector matrix \n    sst2_xt=pd.DataFrame(sst2_xt)\n    sst2_yt=pd.DataFrame(sst2_yt)\n    sst2_xd=pd.DataFrame(sst2_xd)\n    sst2_yd=pd.DataFrame(sst2_yd)\n    sentence1=np.zeros((sst2_xd.shape[0],300))\n    for i,j in enumerate(sst2_xd[0]):\n        sentence1[i]=j[0:300]\n    sentence1.shape\n    \n    sentence=np.zeros((sst2_xt.shape[0],300))\n    for i,j in enumerate(sst2_xt[0]):\n        sentence[i]=j[0:300]\n    sentence.shape\n    \n#     for i,j in range(sst2_yt)\n    print(sst2_yt.shape[1])\n    x_train=sentence\n    y_train=pd.get_dummies(sst2_yt)\n    x_test=sentence1\n    y_test=pd.get_dummies(sst2_yd)\n    print(y_train.shape[1])\n    \n\n    model=Sequential()\n    model.add(Dense(50,input_dim=x_train.shape[1],activation='relu'))\n    model.add(Dense(20,activation='relu'))\n    model.add(Dense(10,activation='relu'))\n    model.add(Dense(y_train.shape[1],activation='softmax'))\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    my_callbacks = [keras.callbacks.EarlyStopping(patience=20)]\n    model.fit(x_train,y_train,epochs=10,batch_size=50,callbacks=my_callbacks,validation_data=(x_test,y_test))\n    _,accuracy=model.evaluate(x_train,y_train)\n\n    output=model.predict(x_test)\n    result=np.zeros((output.shape[0],output.shape[1]))\n    for i,j in enumerate(output):\n        result[i][np.argmax(output[i])]=1\n    accuracy=accuracy_score(y_test,result)\n    print(f\"test accuracy on sst2 dataset: {accuracy}\")\n       \n    file2 = open('../input/dlnlp-assig2/CR.x.test.txt', 'r') \n    listsy = file2.readlines()\n    x_gen=[]\n    for i in range(len(listsy)):\n        x_gen.append(listsy[i].strip('\\n'))\n    x_gen=pd.DataFrame(x_gen)\n    x_gen[0]=Clean(x_gen[0])\n    x_gen[0]=Lemmatization(x_gen[0])\n    x_gen[0]=StopWords(x_gen[0])\n    x_gen[0]=SentenceVector(x_gen[0])\n\n    sentence12=np.zeros((x_gen.shape[0],300))\n\n    for i,j in enumerate(x_gen[0]):\n        sentence12[i]=j[0:300]\n    x_test1=sentence12\n\n    output=model.predict(x_test1)\n    result=np.zeros((output.shape[0],output.shape[1]))\n    for i,j in enumerate(output):\n        result[i][numpy.argmax(output[i])]=1\n\n    result=pd.DataFrame(result)\n    result.columns=y_test.columns\n    df = result.copy()\n    df['columns'] = df.apply(lambda x: df.columns[x.argmax()][2:], axis=1)\n    df['columns'].to_csv(\"16701.CR.charagram.y.test.txt\",header=False,index=False)    \nCR()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.get_dummies(sst1_yd).head)\npd.get_dummies(sst1_yt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=pd.DataFrame(sentence)\ny_train=pd.get_dummies(sst1_yt)\nx_test=pd.DataFrame(sentence1)\ny_test=pd.get_dummies(sst1_yd)\n\nmodel=Sequential()\nmodel.add(Dense(50,input_dim=x_train.shape[1],activation='relu'))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(y_train.shape[1],activation='softmax'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(x_train,y_train,epochs=14,batch_size=50)\n\noutput=model.predict(x_test)\nresult=np.zeros((output.shape[0],output.shape[1]))\nfor i,j in enumerate(output):\n    result[i][np.argmax(output[i])]=1\naccuracy=accuracy_score(y_test,result)\nprint(f\"test accuracy on sst1 dataset: {accuracy}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating output files"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_gen=pd.read_csv(\"../input/dlnlp-assig2/CR.x.test.txt\",header=None)\nx_gen[0]=Clean(x_gen[0])\nx_gen[0]=Lemmatization(x_gen[0])\nx_gen[0]=StopWords(x_gen[0])\nx_gen[0]=SentenceVector(x_gen[0])\n\nsentence12=np.zeros((x_gen.shape[0],300))\n\nfor i,j in enumerate(x_gen[0]):\n    sentence12[i]=j[0:300]\nx_test1=sentence12\n\noutput=model.predict(x_test1)\nresult=np.zeros((output.shape[0],output.shape[1]))\nfor i,j in enumerate(output):\n    result[i][numpy.argmax(output[i])]=1\n\nresult=pd.DataFrame(result)\nresult.columns=y_test.columns\ndf = result.copy()\ndf['columns'] = df.apply(lambda x: df.columns[x.argmax()][2:], axis=1)\ndf['columns'].to_csv(\"16701.MR.y.test.txt\",header=False,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = y_test.copy()\ndf['columns'] = df.apply(lambda x: df.columns[x.argmax()][2:], axis=1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## YOON KIN CNN"},{"metadata":{},"cell_type":"markdown","source":"## Converting sentence into 2D matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Sent2DVec(data):\n    Sent2D=np.zeros((data.shape[0],30,300))\n    for i,sentence in enumerate(data):\n        vector=np.zeros((30,200))\n        for j,word in enumerate(sentence):\n            if word in glovemodel.keys():\n                vector[j]=glovemodel[word]\n            Sent2D[i]=vector\n    return Sent2D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=Sent2DVec(sst1_xt[0])\na=np.reshape(a,(a.shape[0],a.shape[1],a.shape[2],1))\na.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D\n\na=Sent2DVec(sst1_xt[0])\na=np.reshape(a,(a.shape[0],a.shape[1],a.shape[2],1))\nx_train=a\n\ny_train=pd.get_dummies(sst1_yt)\nsst1_xd=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-1.x.dev.txt\",header=None)\nsst1_xd[0]=Clean(sst1_xd[0])\nsst1_xd[0]=Lemmatization(sst1_xd[0])\nsst1_xd[0]=StopWords(sst1_xd[0])\na=Sent2DVec(sst1_xd[0])\na=np.reshape(a,(a.shape[0],a.shape[1],a.shape[2],1))\nx_test=a\ny_test=pd.get_dummies(sst1_yd)\nmodel=Sequential()\nmodel.add(Conv2D(300,(2,200),activation='relu',input_shape=(30,200,1)))\nmodel.add(layers.MaxPooling2D((29, 1)))\nmodel.add(Dense(20,activation='relu'))\nmodel.add(Dense(5,activation='softmax'))\n#callbacks\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(x_train,y_train,epochs=12,batch_size=1,callbacks=[callback],validation_data=(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing CNN on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def CnnTest():\n    sst1_xd=pd.read_csv(\"/kaggle/input/dlnlp-assig2/DLNLP assig2/SST-1.x.dev.txt\",header=None)\n    sst1_xd[0]=Clean(sst1_xd[0])\n    sst1_xd[0]=Lemmatization(sst1_xd[0])\n    sst1_xd[0]=StopWords(sst1_xd[0])\n    a=Sent2DVec(sst1_xd[0])\n    a=np.reshape(a,(a.shape[0],a.shape[1],a.shape[2],1))\n    x_test=a\n    y_test=pd.get_dummies(sst1_yd)\n    output=model.predict(x_test)\n    output=np.reshape(output,(output.shape[0],output.shape[3]))\n    result=np.zeros((output.shape[0],output.shape[1]))\n    for i,j in enumerate(output):\n        result[i][np.argmax(output[i])]=1\n    accuracy=accuracy_score(y_test,result)\n    print(f\"test accuracy on sst1 dataset: {accuracy}\")\n    return x_test.shape\n    \nCnnTest()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}